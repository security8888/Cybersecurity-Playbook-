Log Analysis is the process of reviewing computer generated event logs.  The reason for reviewing event logs is to identify bugs, security threats or other risks.  Log analysis also can be used to fulfill regulations or review user habits.  Organizations must obey specific regulations that control how data is archived and analyzed.  An event log is a comprehensive file that describes the activity occurring within an operating system, software or device.  The system administrator can configure the event log to report on any information they choose, this includes messages, error reports, file requests, file transfers and login and logout requests.  Event logs are timestamped making it easier to establish a trail of activity in lieu of a system failure, breach or other issue.  Log analysis is beneficial to an organization because it aids in troubleshooting, enhancing cybersecurity and improves a customerâ€™s experience.

Log analysis is done within a log management system; a software that gathers, sorts and stores log data and event logs from a variety of sources making it a single point from which to access all relevant endpoint, network and application data.  Log files are fully indexed and searchable.  Ingestion is installing a log collector to gather data from various sources across a network infrastructure.  Centralization is amassing all log data in a single location in a standardized format.  Search and analysis are using a combination of AI/ML-enabled log analysis and human resources to review and analyze known errors, suspicious activity or other abnormalities within the network.  It is incredibly important to automate as much of the log file as possible because of the vast amounts of information gathered by the log analysis software.  Creating graphical representations of this data can help the IT team to understand log entry, its timing and interrelations.  Alerts can be configured to be sent out automatically when certain events occur, or certain conditions are not met.  Streamlined reports of all events and an insightful interface should be created from the log analysis to better aid in the security of the network. 

Techniques used in log analysis are normalization, pattern recognition, classification and tagging, correlation analysis, and artificial ignorance.  Normalization ensures all data and attributes, like IP addresses and timestamps are formatted in a consistent manner.  Pattern recognition is filtering events based on a pattern book to separate routine events from abnormal ones.  Classification and tagging is the process of tagging events with keywords and classifying them by group so similar or related events can be reviewed together.  Correlation analysis gathers log data from several different sources and reviews the data as whole using log analytics.  Artificial ignorance is the active disregard for entries that are not relevant to system health or performance.


